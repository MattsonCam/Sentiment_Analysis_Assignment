{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dJO9onyZ1L_t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.layers import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import layers, Input, Model\n",
    "from sklearn.metrics import precision_score, recall_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZYCY-w7h1Ugc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplistic , silly and tedious . \\n', \"it's so laddish and juvenile , only teenage boys could possibly find it funny . \\n\", 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \\n', '[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \\n', 'a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \\n']\n"
     ]
    }
   ],
   "source": [
    "X_pathneg = 'rt-polarity.neg'\n",
    "X_pathpos = 'rt-polarity.pos'\n",
    "\n",
    "with open(X_pathneg, encoding = \"ISO-8859-1\") as file:\n",
    "    X_listneg = file.readlines()\n",
    "\n",
    "with open(X_pathpos, encoding = \"ISO-8859-1\") as file:\n",
    "    X_listpos = file.readlines()\n",
    " \n",
    "X_list = X_listneg + X_listpos\n",
    "print(X_list[:5])\n",
    "y_list = [0]*len(X_listneg) + [1]*len(X_listpos)\n",
    "\n",
    "X_list = [classval[:-1] for classval in X_list]\n",
    "classes = np.unique(y_list)\n",
    "unique_letters = np.unique(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XO7z_Mvd23Zj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 22:16:19.027901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-20 22:16:19.028000: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-20 22:16:19.028085: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (school-pc): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "vectorizer = TextVectorization(max_tokens=20600, output_sequence_length=embed_dim)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(X_list).batch(128) ## Read batches of 128 samples\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lNxSh02l1Vjh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"those outside show business will enjoy a close look at people they don't really want to know . \"]\n",
      " ['the punch lines that miss , unfortunately , outnumber the hits by three-to-one . but death to smoochy keeps firing until the bitter end . ']\n",
      " ['intimate and panoramic . ']\n",
      " ...\n",
      " ['while solondz tries and tries hard , storytelling fails to provide much more insight than the inside column of a torn book jacket . ']\n",
      " [\"the movie doesn't generate a lot of energy . it is dark , brooding and slow , and takes its central idea way too seriously . \"]\n",
      " ['the most anti-human big studio picture since 3000 miles to graceland . ']]\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_to_index = dict(zip(vocab,range(len(vocab))))\n",
    "index_to_vocab = dict(zip(range(len(vocab)),vocab))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, train_size = 7/10, random_state = 1)\n",
    "print(np.array([[s] for s in X_train]))\n",
    "X_train = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
    "X_test = vectorizer(np.array([[s] for s in X_test])).numpy()\n",
    "\n",
    "y_train = to_categorical(y_train).astype(np.int64)\n",
    "y_test = to_categorical(y_test).astype(np.int64)\n",
    "y_test_labels = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P2IfYGar1aiF"
   },
   "outputs": [],
   "source": [
    "class models:\n",
    "\n",
    "  def __init__(self, xtrain, ytrain, embed_layer):\n",
    "    self.X_train = xtrain\n",
    "    self.y_train = ytrain\n",
    "    self.embedding_layer = embed_layer\n",
    "    self.modmetrics = []\n",
    "\n",
    "  def conf_mat(self, y_test):\n",
    "    y_test = np.argmax(y_test, axis = 1)\n",
    "    conf_mat = confusion_matrix(y_test, self.y_pred)\n",
    "    return conf_mat\n",
    "\n",
    "  def get_metrics(self,ytest,ypred):\n",
    "    self.modmetrics.append([precision_score(ytest, ypred), recall_score(ytest, ypred)])\n",
    "    return self\n",
    "\n",
    "  def get_pred(self, X_test, y_test):\n",
    "    modpreds = np.argmax(self.savedmodel.predict(X_test), axis = 1)\n",
    "    y_test = np.argmax(y_test, axis = 1)\n",
    "    self.get_metrics(y_test,modpreds)\n",
    "    self.y_pred = modpreds\n",
    "    return self\n",
    "\n",
    "  def lstm_mod(self):\n",
    "    classes = self.y_train.shape[1]\n",
    "    int_sequences_input = Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = self.embedding_layer(int_sequences_input)\n",
    "    x = layers.Bidirectional(layers.LSTM(20))(embedded_sequences)\n",
    "    preds = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model1 = Model(int_sequences_input, preds)\n",
    "    #model1.summary()\n",
    "\n",
    "    model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    model1.fit(self.X_train, self.y_train, batch_size=128, epochs=2)\n",
    "    \n",
    "    self.savedmodel = model1\n",
    "\n",
    "    return self\n",
    "\n",
    "  def gru_mod(self):\n",
    "    classes = self.y_train.shape[1]    \n",
    "    int_sequences_input = Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = self.embedding_layer(int_sequences_input)\n",
    "    x = layers.Bidirectional(layers.GRU(20))(embedded_sequences)\n",
    "    preds = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model2 = Model(int_sequences_input, preds)\n",
    "    #model2.summary()\n",
    "\n",
    "    model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    model2.fit(self.X_train, self.y_train, batch_size=128, epochs=2)\n",
    "    \n",
    "    self.savedmodel = model2\n",
    "\n",
    "    return self\n",
    "\n",
    "  def rnn_mod(self):\n",
    "    classes = self.y_train.shape[1]\n",
    "    int_sequences_input = Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = self.embedding_layer(int_sequences_input)\n",
    "    x = layers.Bidirectional(layers.SimpleRNN(20))(embedded_sequences)\n",
    "    preds = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model3 = Model(int_sequences_input, preds)\n",
    "    #model3.summary()\n",
    "\n",
    "    model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    model3.fit(self.X_train, self.y_train, batch_size=128, epochs=2)\n",
    "\n",
    "    self.savedmodel = model3\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiUanSPq1eU0",
    "outputId": "7d1de9d8-70b9-4e89-a147-e93cb36bdcfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-07 14:49:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2022-03-07 14:49:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-03-07 14:49:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 42s  \n",
      "\n",
      "2022-03-07 14:52:34 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oNLl_BI11nVi"
   },
   "outputs": [],
   "source": [
    "def create_embed(filepath, myvocab, myembed_dim):\n",
    "  path_to_glove_file = filepath\n",
    "  vocab = myvocab\n",
    "  embed_dim = myembed_dim\n",
    "  embeddings_index = {}\n",
    "  with open(path_to_glove_file) as f:\n",
    "      for line in f:\n",
    "          word, coefs = line.split(maxsplit=1)\n",
    "          coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "          embeddings_index[word] = coefs\n",
    "\n",
    "  num_tokens = len(vocab) \n",
    "\n",
    "  hits = 0 ## number of words that were found in the pretrained model\n",
    "  misses = 0 ## number of words that were missing in the pretrained model\n",
    "  word_index = dict(zip(vocab, range(len(vocab))))\n",
    "  # Prepare embedding matrix for our word list\n",
    "  embedding_matrix = np.zeros((num_tokens, embed_dim))\n",
    "  for word, i in word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          # Words not found in embedding index will be all-zeros.\n",
    "          # This includes the representation for \"padding\" and \"OOV\"\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "          hits += 1\n",
    "      else:\n",
    "          misses += 1\n",
    "\n",
    "  print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "  embedding_layer = Embedding(num_tokens, embed_dim,\n",
    "                              embeddings_initializer= Constant(embedding_matrix), \n",
    "                              trainable=False,\n",
    "  )\n",
    "\n",
    "  return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "bv1BUsW116uO",
    "outputId": "446be73f-1134-47fa-e31b-35a44da29a49"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9b2ce061c0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/emb.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrnn_obj3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrnn_obj3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrnn_mets3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_obj3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5a1909fd8493>\u001b[0m in \u001b[0;36mcreate_embed\u001b[0;34m(filepath, myvocab, myembed_dim)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyembed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_glove_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/emb.txt'"
     ]
    }
   ],
   "source": [
    "embedding_layer = create_embed(\"drive/MyDrive/emb.txt\", vocab, embed_dim)\n",
    "\n",
    "rnn_obj3 = models(X_train, y_train, embedding_layer).rnn_mod()\n",
    "rnn_obj3.get_pred(X_test, y_test)\n",
    "rnn_mets3 = rnn_obj3.modmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtHlr3zx3ocT"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hURjErSs1xQn"
   },
   "outputs": [],
   "source": [
    "embedding_layer = create_embed(\"glove.6B.100d.txt\", vocab, embed_dim)\n",
    "\n",
    "rnn_obj4 = models(X_train, y_train, embedding_layer).lstm_mod()\n",
    "rnn_obj4.get_pred(X_test, y_test)\n",
    "rnn_mets4 = rnn_obj4.modmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57Io6yGy2aaJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "glove_mat = rnn_obj3.conf_mat(y_test)\n",
    "other_mat = rnn_obj4.conf_mat(y_test)\n",
    "\n",
    "myax = sns.heatmap(glove_mat.T, square = True, annot = True, fmt = 'd', \\\n",
    "cbar = False).set(title='RNN Confusion matrix with GloVE',xlabel='True Label', ylabel='Predicted Label')\n",
    "plt.savefig('glove_conf_mat.png')\n",
    "plt.show()\n",
    "'''\n",
    "myax = sns.heatmap(other_mat.T, square = True, annot = True, fmt = 'd', \\\n",
    "cbar = False).set(title='RNN Confusion matrix with Blank',xlabel='True Label', ylabel='Predicted Label')\n",
    "plt.savefig('other_conf_mat.png')\n",
    "plt.show()\n",
    "'''\n",
    "print(f'\\nThe RNN model\\'s precision is {rnn_mets3[0][0]} and the RNN model\\'s recall is {rnn_mets3[0][1]} using the pretrained glove word embeddings.')\n",
    "print(f'\\nThe RNN model\\'s precision is {rnn_mets4[0][0]} and the RNN model\\'s recall is {rnn_mets4[0][1]} using the other pretrained word embeddings.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIHg_T622l60"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab_assignment_3_p2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
